{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import datetime\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of files in the transcripts folder\n",
    "DIR_FOLDER = \"raw_dataset_v3\"\n",
    "OUTPUT_FOLDER = \"dataset_v3\"\n",
    "transcripts = os.listdir(DIR_FOLDER)\n",
    "\n",
    "# if output folder doesn't exist, create it\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_transcript(data):\n",
    "    subset_texts = []\n",
    "    text = \"\"\n",
    "    start_time = data[\"segments\"][0][\"start\"]\n",
    "    idx = 0\n",
    "\n",
    "    while idx < len(data[\"segments\"]):\n",
    "        segment = data[\"segments\"][idx]\n",
    "        segment_start_time = segment[\"start\"]\n",
    "        segment_end_time = segment[\"end\"]\n",
    "        segment_text = segment[\"text\"]\n",
    "\n",
    "        # add segment_text to text\n",
    "        text += \" \" + segment_text.strip() if text != \"\" else segment_text.strip()\n",
    "\n",
    "        # check if text is longer\n",
    "        if len(text) > 1500:\n",
    "            subset_texts.append([start_time, segment_end_time, text])\n",
    "            start_time = segment_end_time\n",
    "            text = \"\"\n",
    "            idx -= 5\n",
    "        idx += 1\n",
    "    \n",
    "    # add the last text\n",
    "    subset_texts.append([start_time, segment_end_time, text])\n",
    "\n",
    "    #print(\"Starting summarization\")\n",
    "    summary_dict = summarizer(\n",
    "        [subset[2] for subset in subset_texts],\n",
    "        max_length=100,\n",
    "        min_length=0,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    return summary_dict, subset_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamps_to_seconds(timestamps):\n",
    "    # Iterate through each time stamp, check if it's in format H#M#S# or M#S# and convert it to seconds\n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        timestamp = timestamp.split(\":\")\n",
    "        # add the time to the timestamp\n",
    "        timestamps[i] = sum([int(t)*(60**i) for i, t in enumerate(timestamp[::-1])])\n",
    "\n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(transcripts)\n",
    "\n",
    "for transcript in tqdm(transcripts):\n",
    "    subset_texts = []\n",
    "    #print(f\"Processing {transcript.replace('.json', '')}\")\n",
    "    # open the json file\n",
    "    try:\n",
    "        with open(f\"{DIR_FOLDER}/{transcript}\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # if the transcript already is in either OUTPUT_FOLDER\n",
    "    files = os.listdir(OUTPUT_FOLDER)\n",
    "    # check if transcript name is in any file, if so, skip\n",
    "    if any(transcript.replace(\".json\", \"\") in file for file in files):\n",
    "        continue\n",
    "\n",
    "    exists_timestamps = False if len(data[\"timestamps\"]) == 0 else True\n",
    "\n",
    "    if not exists_timestamps:\n",
    "        continue\n",
    "\n",
    "    summary_dict, subset_texts = summarize_transcript(data)\n",
    "    summarized_text_string = \"Given summarizations we want to know the keyword for each summary in the list.\\n\\n\"\n",
    "    label_string = \"\"\n",
    "    dataset = []\n",
    "    json_idx = 0\n",
    "\n",
    "    time_stamps = convert_timestamps_to_seconds(data['timestamps'])\n",
    "    keywords = data['keywords']\n",
    "    if time_stamps[0] != 0:\n",
    "        time_stamps.insert(0, 0)\n",
    "        keywords.insert(0, \"Introduction\")\n",
    "\n",
    "    for idx, ((start, end, t), summary_d) in enumerate(zip(subset_texts, summary_dict)):\n",
    "        # we want to get the first index where start_time is greater than timestamps[i]\n",
    "        i = np.where([start>=tim for tim in time_stamps])[0].max()\n",
    "        keyword = keywords[i]\n",
    "        \n",
    "        keyword = keyword.strip().strip(\"|\").strip()\n",
    "        summary = summary_d[\"summary_text\"]\n",
    "        writing_idx = idx % 10\n",
    "        summarized_text_string += f\"{writing_idx + 1}. {summary}\\n\"\n",
    "        label_string += f\"{writing_idx + 1}. {keyword}\\n\"\n",
    "\n",
    "        if (idx + 1) % 10 == 0 or idx == len(summary_dict) - 1:\n",
    "            summarized_text_string += f\"\\n\\n###\\n\\nKEYWORDS:\\n\"\n",
    "            dataset.append({\"prompt\": summarized_text_string, \"completion\": label_string})\n",
    "\n",
    "           \n",
    "            with open(f\"{OUTPUT_FOLDER}/{json_idx}_{transcript}\", \"w\") as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"prompt\": summarized_text_string,\n",
    "                        \"completion\": label_string,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "            json_idx += 1\n",
    "            summarized_text_string = \"Given summarizations we want to know the keyword for each summary in the list.\\n\\n\"\n",
    "            label_string = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ythelper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6c81a6c2acde1c8a96f2afe16e136fb286d5a04c0643ed36c3def4138fcfe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
